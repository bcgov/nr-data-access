# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  repository: ~ # the repository where the images are stored.
  registry: ghcr.io # the registry where the images are stored. override during runtime for other registry at global level or individual level.
  tag: ~
  autoscaling: true
  secrets:
    enabled: false
    databasePassword: default
    databaseUser: default
    databaseName: default
    SERVER_KC_CLIENTID: default
    SERVER_KC_CLIENTSECRET: default
    SC_CS_CHES_CLIENT_ID: default
    SC_CS_CHES_CLIENT_SECRET: default
    SC_CS_CDOGS_CLIENT_ID: default
    SC_CS_CDOGS_CLIENT_SECRET: default
    FILES_OBJECTSTORAGE_ACCESSKEYID: default
    APITOKEN: default
    annotation:
      helm.sh/policy: "keep"
  domain: "apps.silver.devops.gov.bc.ca" # it is required, apps.silver.devops.gov.bc.ca for silver cluster
  openshiftImageRegistry: "image-registry.openshift-image-registry.svc:5000"
  databaseAlias: bitnami-pg # this is the alias for bitnami postgres, change it based on db type(crunchy,patroni...) and alias used in the chart.

workflows:
  enabled: true
  deployment: # can be either a statefulSet or a deployment not both
    enabled: true
  containers:
    - name: workflows
      registry: '{{ .Values.global.registry }}'
      repository: '{{ .Values.global.repository }}' # example, it includes registry and repository
      image: workflows # the exact component name, be it backend, api-1 etc...
      tag: '{{ .Values.global.tag }}' # the tag of the image, it can be latest, 1.0.0 etc..., or the sha256 hash
      env:
        fromGlobalSecret:
          - name: POSTGRES_DATABASE
            secretName: '{{ .Release.Name }}-crunchy-pguser-nrdataacessworkflows'
            key: dbname
          - name: POSTGRES_HOST
            secretName: '{{ .Release.Name }}-crunchy-pguser-nrdataacessworkflows'
            key: pgbouncer-host
          - name: POSTGRES_USER
            secretName: '{{ .Release.Name }}-crunchy-pguser-nrdataacessworkflows'
            key: user
          - name: POSTGRES_PASSWORD
            secretName: '{{ .Release.Name }}-crunchy-pguser-nrdataacessworkflows'
            key: password

      ports:
        - name: http
          containerPort: 3000
          protocol: TCP
      resources: # this is optional
        limits:
          cpu: 100m
          memory: 150Mi
        requests:
          cpu: 30m
          memory: 50Mi
      readinessProbe:
        httpGet:
          path: /q/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 2
        timeoutSeconds: 2
        successThreshold: 1
        failureThreshold: 30
      livenessProbe:
        successThreshold: 1
        failureThreshold: 3
        httpGet:
          path: /q/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5

frontend:
  enabled: false
  deployment: # can be either a statefulSet or a deployment not both
    enabled: true
  configmap:
    enabled: true
    data: # below is just for example.
      VITE_CONTACT: 'omprakash.2.mishra@gov.bc.ca'
      VITE_HOWTOURL: 'https://howto.gov.bc.ca'
      VITE_CHEFSTOURURL: 'https://chefstour.gov.bc.ca'
      FRONTEND_APIPATH: api/v1
      VITE_FRONTEND_BASEPATH: /app
      FRONTEND_BASEPATH: /app
      FRONTEND_ENV: dev
      FRONTEND_KC_REALM: chefs
      FRONTEND_KC_SERVERURL: https://dev.loginproxy.gov.bc.ca/auth
      SC_CS_CHES_ENDPOINT: https://ches-dev.api.gov.bc.ca/api
      SC_CS_CDOGS_ENDPOINT: https://cdogs-dev.api.gov.bc.ca/api
      SC_CS_CHES_TOKEN_ENDPOINT: https://dev.loginproxy.gov.bc.ca/auth/realms/comsvcauth/protocol/openid-connect/token
      SC_CS_CDOGS_TOKEN_ENDPOINT: https://dev.loginproxy.gov.bc.ca/auth/realms/comsvcauth/protocol/openid-connect/token
      SERVER_APIPATH: /api/v1
      SERVER_BASEPATH: /app
      SERVER_BODYLIMIT: 30mb
      SERVER_KC_PUBLICKEY: $PUBLIC_KEY
      SERVER_KC_REALM: standard
      SERVER_KC_SERVERURL: https://dev.loginproxy.gov.bc.ca/auth
      SERVER_LOGLEVEL:  http
      SERVER_PORT: 8080
      FILES_UPLOADS_DIR:
      FILES_UPLOADS_ENABLED: true
      FILES_UPLOADS_FILECOUNT: 1
      FILES_UPLOADS_FILEKEY: files
      FILES_UPLOADS_FILEMAXSIZE: 25MB
      FILES_UPLOADS_FILEMINSIZE: 0KB
      FILES_UPLOADS_PATH: files
      FILES_PERMANENT: objectStorage
      FILES_LOCALSTORAGE_PATH:
      FILES_OBJECTSTORAGE_BUCKET: '{{ .Values.global.objectstorage.bucket }}'
      FILES_OBJECTSTORAGE_ENDPOINT: https://nrs.objectstore.gov.bc.ca
      FILES_OBJECTSTORAGE_KEY: chefs/dev/
      NODE_ENV: production
      VITE_CHEFS_GEO_ADDRESS_APIURL: https://geocoder.api.gov.bc.ca/addresses.json
      VITE_CHEFS_ADVANCE_GEO_ADDRESS_APIURL: https://geocoder.api.gov.bc.ca/addresses.json
      VITE_BC_GEO_ADDRESS_APIURL: https://geocoder.api.gov.bc.ca/addresses.json
  containers:
    - name: frontend
      registry: '{{ .Values.global.registry }}'
      repository: '{{ .Values.global.repository }}' # example, it includes registry and repository
      image: frontend # the exact component name, be it backend, api-1 etc...
      tag: '{{ .Values.global.tag }}' # the tag of the image, it can be latest, 1.0.0 etc..., or the sha256 hash
      envFrom:
        - configMapRef:
        - secretRef:
      env:
        fromGlobalSecret:
          - name: DB_DATABASE
            secretName: '{{ .Release.Name }}-crunchy-pguser-nrdataaccessfrontend'
            key: dbname
          - name: DB_HOST
            secretName: '{{ .Release.Name }}-crunchy-pguser-nrdataaccessfrontend'
            key: pgbouncer-host
          - name: DB_USERNAME
            secretName: '{{ .Release.Name }}-crunchy-pguser-nrdataaccessfrontend'
            key: user
          - name: DB_PASSWORD
            secretName: '{{ .Release.Name }}-crunchy-pguser-nrdataaccessfrontend'
            key: password

      ports:
        - name: http
          containerPort: 3000
          protocol: TCP
        - name: http2
          containerPort: 3001
          protocol: TCP
      resources: # this is optional
        limits:
          cpu: 100m
          memory: 150Mi
        requests:
          cpu: 30m
          memory: 50Mi
      readinessProbe:
        httpGet:
          path: /health
          port: 3001
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 2
        timeoutSeconds: 2
        successThreshold: 1
        failureThreshold: 30
      livenessProbe:
        successThreshold: 1
        failureThreshold: 3
        httpGet:
          path: /health
          port: 3001
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 7
    targetCPUUtilizationPercentage: 80 # this percentage from request cpu
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 10
            periodSeconds: 60
          - type: Pods
            value: 2
            periodSeconds: 60
        selectPolicy: Min
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
          - type: Percent
            value: 100
            periodSeconds: 30
          - type: Pods
            value: 2
            periodSeconds: 30
        selectPolicy: Max
  service:
    enabled: true
    type: ClusterIP
    ports:
      - name: http
        port: 80
        targetPort: 3000 # the container port where the application is listening on
        protocol: TCP
  route:
    enabled: true
    host: "{{ .Release.Name }}-frontend.{{ .Values.global.domain }}"
    targetPort: http # look at line#164 refer to the name.

crunchy: # enable it for TEST and PROD, for PR based pipelines simply use single postgres
  enabled: true
  users:
    - name: postgres
      databases:
        - postgres
    - name: nrdataaccessfrontend
      databases:
        - nrdataaccessfrontend
    - name: nrdataacessworkflows
      databases:
        - nrdataacessworkflows
    - name: nrdataacessreporting
      databases:
        - nrdataacessreporting

  postgresVersion: 15
  imagePullPolicy: Always
  instances:
    name: ha # high availability
    replicas: 2 # 2 or 3 for high availability in TEST and PROD.
    metadata:
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9187'
    dataVolumeClaimSpec:
      storage: 420Mi
      storageClassName: netapp-block-standard
    requests:
      cpu: 25m
      memory: 256Mi
    limits:
      cpu: 100m
      memory: 512Mi
    replicaCertCopy:
      requests:
        cpu: 1m
        memory: 32Mi
      limits:
        cpu: 50m
        memory: 64Mi

  pgBackRest:
    image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
    retention: "1" # Ideally a larger number such as 30 backups/days
    # If retention-full-type set to 'count' then the oldest backups will expire when the number of backups reach the number defined in retention
    # If retention-full-type set to 'time' then the number defined in retention will take that many days worth of full backups before expiration
    retentionFullType: count
    repos:
      schedules:
        full: 0 8 * * *
        incremental: 0 0,4,12,16,20 * * *
      volume:
        accessModes: "ReadWriteOnce"
        storage: 64Mi
        storageClassName: netapp-file-backup
    repoHost:
      requests:
        cpu: 1m
        memory: 64Mi
      limits:
        cpu: 50m
        memory: 128Mi
    sidecars:
      requests:
        cpu: 1m
        memory: 64Mi
      limits:
        cpu: 50m
        memory: 128Mi

  patroni:
    postgresql:
      pg_hba: "host all all 0.0.0.0/0 md5"
      parameters:
        shared_buffers: 16MB # default is 128MB; a good tuned default for shared_buffers is 25% of the memory allocated to the pod
        wal_buffers: "64kB" # this can be set to -1 to automatically set as 1/32 of shared_buffers or 64kB, whichever is larger
        min_wal_size: 32MB
        max_wal_size: 64MB # default is 1GB
        max_slot_wal_keep_size: 128MB # default is -1, allowing unlimited wal growth when replicas fall behind

  proxy:
    pgBouncer:
      image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
      replicas: 1
      requests:
        cpu: 1m
        memory: 64Mi
      limits:
        cpu: 50m
        memory: 128Mi

  # Postgres Cluster resource values:
  pgmonitor:
    enabled: false
    exporter:
      image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
      requests:
        cpu: 1m
        memory: 64Mi
      limits:
        cpu: 50m
        memory: 128Mi

keycloak:
  enabled: true
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false
  livenessProbe:
    enabled: true
    initialDelaySeconds: 300
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30
    successThreshold: 1
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 50
    successThreshold: 1
  resources:
    requests:
      cpu: 100m
      memory: 300Mi
    limits:
      cpu: 250m
      memory: 500Mi
  postgresql:
    enabled: true
    shmVolume:
      enabled: false
    primary:
      podSecurityContext:
        enabled: false
      containerSecurityContext:
        enabled:
          false
      resources:
        requests:
          cpu: 100m
          memory: 250Mi
        limits:
          cpu: 250m
          memory: 500Mi
      persistence:
        size: 150Mi
        storageClass: netapp-block-standard


